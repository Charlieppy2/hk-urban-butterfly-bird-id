"""
æ¨¡å‹è¨­ç½®èˆ‡åŠ è¼‰æ¨¡å¡Š
ç”¨æ–¼åŠ è¼‰ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆé è¨“ç·´æ¨¡å‹

é¸æ“‡çš„æ¨¡å‹ï¼šGPT-2 ä¸­æ–‡ç‰ˆæœ¬ (uer/gpt2-chinese-cluecorpussmall)
é¸æ“‡ç†ç”±ï¼š
1. å°ˆç‚ºä¸­æ–‡è¨­è¨ˆï¼Œå°ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆæ•ˆæœå¥½
2. æ¨¡å‹å¤§å°é©ä¸­ï¼ˆç´„500MBï¼‰ï¼Œæ˜“æ–¼éƒ¨ç½²
3. åŸºæ–¼Transformeræ¶æ§‹ï¼Œæ˜“æ–¼å¾®èª¿
4. åœ¨ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆä»»å‹™ä¸Šè¡¨ç¾è‰¯å¥½
5. æœ‰è±å¯Œçš„ç¤¾å€æ”¯æŒå’Œæ–‡æª”
"""

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import os

def load_model_and_tokenizer(model_path=None, device='cpu'):
    """
    åŠ è¼‰GPT-2ä¸­æ–‡æ¨¡å‹å’Œåˆ†è©å™¨
    
    Args:
        model_path: é è¨“ç·´æ¨¡å‹è·¯å¾‘ï¼ˆå¦‚æœç‚ºNoneï¼Œå‰‡ä½¿ç”¨é»˜èªçš„é è¨“ç·´æ¨¡å‹ï¼‰
        device: é‹è¡Œè¨­å‚™ ('cpu' æˆ– 'cuda')
    
    Returns:
        model: åŠ è¼‰çš„æ¨¡å‹
        tokenizer: åˆ†è©å™¨
    """
    print("ğŸ”„ æ­£åœ¨åŠ è¼‰æ¨¡å‹å’Œåˆ†è©å™¨...")
    
    # å¦‚æœæä¾›äº†æœ¬åœ°æ¨¡å‹è·¯å¾‘ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹
    if model_path and os.path.exists(model_path):
        print(f"ğŸ“‚ å¾æœ¬åœ°è·¯å¾‘åŠ è¼‰æ¨¡å‹: {model_path}")
        model = GPT2LMHeadModel.from_pretrained(model_path)
        tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    else:
        # ä½¿ç”¨Hugging Faceä¸Šçš„ä¸­æ–‡GPT-2æ¨¡å‹
        # å˜—è©¦å¤šå€‹æ¨¡å‹åç¨±ï¼Œé¸æ“‡å¯ç”¨çš„
        model_names = [
            "uer/gpt2-chinese-cluecorpussmall",
            "gpt2",  # è‹±æ–‡GPT-2ä½œç‚ºå‚™é¸
        ]
        
        model_loaded = False
        for model_name in model_names:
            try:
                print(f"ğŸ“¥ å˜—è©¦å¾Hugging Faceä¸‹è¼‰æ¨¡å‹: {model_name}")
                tokenizer = GPT2Tokenizer.from_pretrained(model_name)
                model = GPT2LMHeadModel.from_pretrained(model_name)
                print(f"âœ… æˆåŠŸåŠ è¼‰æ¨¡å‹: {model_name}")
                model_loaded = True
                break
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•åŠ è¼‰ {model_name}: {e}")
                continue
        
        if not model_loaded:
            print("âŒ æ‰€æœ‰æ¨¡å‹åŠ è¼‰å¤±æ•—")
            print("ğŸ’¡ æç¤ºï¼šè«‹æª¢æŸ¥ç¶²çµ¡é€£æ¥æˆ–æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹")
            raise Exception("ç„¡æ³•åŠ è¼‰ä»»ä½•é è¨“ç·´æ¨¡å‹")
    
    # è¨­ç½®pad_tokenï¼ˆGPT-2é»˜èªæ²’æœ‰pad_tokenï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # å°‡æ¨¡å‹ç§»åˆ°æŒ‡å®šè¨­å‚™
    model = model.to(device)
    model.eval()  # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
    
    print(f"âœ… æ¨¡å‹å’Œåˆ†è©å™¨åŠ è¼‰æˆåŠŸ (è¨­å‚™: {device})")
    print(f"   æ¨¡å‹åƒæ•¸æ•¸é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    return model, tokenizer

def prepare_input(species, category, tokenizer, max_length=128):
    """
    æº–å‚™æ¨¡å‹è¼¸å…¥
    
    Args:
        species: ç‰©ç¨®åç¨±
        category: é¡åˆ¥ï¼ˆå¦‚fun_fact, behavior, habitatï¼‰
        tokenizer: åˆ†è©å™¨
        max_length: æœ€å¤§é•·åº¦
    
    Returns:
        input_ids: ç·¨ç¢¼å¾Œçš„è¼¸å…¥ID
        attention_mask: æ³¨æ„åŠ›æ©ç¢¼
    """
    # æ§‹å»ºè¼¸å…¥æ–‡æœ¬ï¼šç‰©ç¨® + é¡åˆ¥
    # æ ¼å¼ï¼š"{species}çš„{category}ï¼š"
    input_text = f"{species}çš„{category}ï¼š"
    
    # ä½¿ç”¨åˆ†è©å™¨ç·¨ç¢¼
    encoded = tokenizer.encode_plus(
        input_text,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    return encoded['input_ids'], encoded['attention_mask']

if __name__ == "__main__":
    # æ¸¬è©¦åŠ è¼‰æ¨¡å‹
    print("=" * 60)
    print("ğŸ§ª æ¸¬è©¦æ¨¡å‹åŠ è¼‰")
    print("=" * 60)
    
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ä½¿ç”¨è¨­å‚™: {device}")
        
        model, tokenizer = load_model_and_tokenizer(device=device)
        
        # æ¸¬è©¦è¼¸å…¥æº–å‚™
        print("\nğŸ§ª æ¸¬è©¦è¼¸å…¥æº–å‚™...")
        input_ids, attention_mask = prepare_input("è‡ºç£è—éµ²", "fun_fact", tokenizer)
        print(f"âœ… è¼¸å…¥æº–å‚™æˆåŠŸ")
        print(f"   è¼¸å…¥å½¢ç‹€: {input_ids.shape}")
        print(f"   è¼¸å…¥æ–‡æœ¬: è‡ºç£è—éµ²çš„fun_factï¼š")
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

