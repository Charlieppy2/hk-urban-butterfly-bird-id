"""
Train Intent Classification Model for AI Assistant
This model classifies user questions into different intent categories
"""

import json
import os
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pickle
from collections import Counter

# Intent categories based on knowledge base
INTENT_CATEGORIES = [
    'greetings',
    'identification_tips',
    'observation_time',
    'photo_tips',
    'species_info',
    'confidence',
    'habitat',
    'system_info',
    'help',
    'default'
]

def load_training_data(knowledge_base_path='knowledge_base.json'):
    """Load training data from knowledge base"""
    if not os.path.exists(knowledge_base_path):
        print(f"âŒ Knowledge base not found: {knowledge_base_path}")
        return None, None
    
    with open(knowledge_base_path, 'r', encoding='utf-8') as f:
        knowledge_base = json.load(f)
    
    texts = []
    labels = []
    
    # Extract patterns and their categories
    for category, data in knowledge_base.items():
        if category == 'default':
            continue
        
        patterns = data.get('patterns', [])
        for pattern in patterns:
            texts.append(pattern.lower())
            labels.append(category)
    
    # Add default examples if needed
    if 'default' in knowledge_base:
        default_patterns = [
            'hello', 'hi', 'help', 'what can you do',
            'ä½ å¥½', 'å¹«åŠ©', 'èƒ½åšä»€éº¼'
        ]
        for pattern in default_patterns:
            texts.append(pattern.lower())
            labels.append('default')
    
    return texts, labels

def create_synthetic_training_data():
    """Create synthetic training data for better coverage"""
    synthetic_data = {
        'greetings': [
            'hello', 'hi', 'hey', 'good morning', 'good afternoon',
            'ä½ å¥½', 'å—¨', 'æ—©æ™¨', 'åˆå®‰', 'å“ˆå›‰'
        ],
        'identification_tips': [
            'how to identify', 'identification tips', 'how to tell',
            'distinguish', 'recognize', 'å¦‚ä½•è­˜åˆ¥', 'è¾¨è­˜', 'è­˜åˆ¥',
            'æ€éº¼åˆ†è¾¨', 'å¦‚ä½•åˆ†è¾¨', 'è¾¨åˆ¥', 'è­˜åˆ¥æŠ€å·§'
        ],
        'observation_time': [
            'when', 'best time', 'season', 'time of day',
            'ä»€éº¼æ™‚å€™', 'æœ€ä½³æ™‚é–“', 'å­£ç¯€', 'æ™‚é–“'
        ],
        'photo_tips': [
            'photo', 'camera', 'picture', 'image quality', 'how to take',
            'æ‹ç…§', 'æ”å½±', 'ç…§ç‰‡', 'åœ–åƒè³ªé‡', 'å¦‚ä½•æ‹æ”'
        ],
        'species_info': [
            'species', 'types', 'kinds', 'varieties', 'information',
            'ç‰©ç¨®', 'ç¨®é¡', 'é¡å‹', 'ä¿¡æ¯'
        ],
        'confidence': [
            'confidence', 'accurate', 'reliable', 'trust', 'accuracy',
            'ç½®ä¿¡åº¦', 'æº–ç¢º', 'å¯é ', 'ä¿¡ä»»'
        ],
        'habitat': [
            'where', 'habitat', 'location', 'find', 'spot',
            'å“ªè£¡', 'æ£²æ¯åœ°', 'ä½ç½®', 'æ‰¾åˆ°', 'åœ°é»'
        ],
        'system_info': [
            'system', 'model', 'accuracy', 'how does it work',
            'ç³»çµ±', 'æ¨¡å‹', 'æº–ç¢ºåº¦', 'å¦‚ä½•å·¥ä½œ'
        ],
        'help': [
            'help', 'what can you do', 'capabilities', 'assist',
            'å¹«åŠ©', 'èƒ½åšä»€éº¼', 'åŠŸèƒ½', 'å”åŠ©'
        ],
        'default': [
            'thanks', 'thank you', 'ok', 'okay', 'yes', 'no',
            'è¬è¬', 'æ„Ÿè¬', 'å¥½çš„', 'æ˜¯', 'å¦'
        ]
    }
    
    texts = []
    labels = []
    
    for category, patterns in synthetic_data.items():
        for pattern in patterns:
            texts.append(pattern.lower())
            labels.append(category)
    
    return texts, labels

def train_model(texts, labels, model_type='naive_bayes'):
    """Train intent classification model"""
    if len(texts) == 0 or len(labels) == 0:
        print("âŒ No training data available")
        return None, None
    
    print(f"ğŸ“Š Training data: {len(texts)} examples")
    print(f"ğŸ“Š Categories: {Counter(labels)}")
    
    # Vectorize text
    vectorizer = TfidfVectorizer(
        max_features=1000,
        ngram_range=(1, 2),  # Unigrams and bigrams
        stop_words='english',
        min_df=1,
        max_df=0.95
    )
    
    X = vectorizer.fit_transform(texts)
    y = np.array(labels)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # Train model
    if model_type == 'naive_bayes':
        model = MultinomialNB(alpha=0.1)
    elif model_type == 'logistic':
        model = LogisticRegression(max_iter=1000, random_state=42)
    else:
        model = MultinomialNB(alpha=0.1)
    
    print(f"ğŸ”„ Training {model_type} model...")
    model.fit(X_train, y_train)
    
    # Evaluate
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"âœ… Model trained successfully!")
    print(f"ğŸ“Š Accuracy: {accuracy:.2%}")
    print(f"\nğŸ“‹ Classification Report:")
    print(classification_report(y_test, y_pred, target_names=INTENT_CATEGORIES))
    
    return model, vectorizer

def save_model(model, vectorizer, model_path='intent_classifier_model.pkl', 
               vectorizer_path='intent_vectorizer.pkl'):
    """Save trained model and vectorizer"""
    try:
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)
        print(f"âœ… Model saved to: {model_path}")
        
        with open(vectorizer_path, 'wb') as f:
            pickle.dump(vectorizer, f)
        print(f"âœ… Vectorizer saved to: {vectorizer_path}")
        
        return True
    except Exception as e:
        print(f"âŒ Error saving model: {e}")
        return False

def load_model(model_path='intent_classifier_model.pkl',
               vectorizer_path='intent_vectorizer.pkl'):
    """Load trained model and vectorizer"""
    try:
        if not os.path.exists(model_path) or not os.path.exists(vectorizer_path):
            return None, None
        
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        
        with open(vectorizer_path, 'rb') as f:
            vectorizer = pickle.load(f)
        
        print(f"âœ… Model loaded from: {model_path}")
        return model, vectorizer
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None, None

def predict_intent(model, vectorizer, text):
    """Predict intent for a given text"""
    if model is None or vectorizer is None:
        return 'default', 0.0
    
    try:
        text_vectorized = vectorizer.transform([text.lower()])
        prediction = model.predict(text_vectorized)[0]
        probabilities = model.predict_proba(text_vectorized)[0]
        confidence = max(probabilities)
        
        return prediction, confidence
    except Exception as e:
        print(f"âš ï¸ Error in prediction: {e}")
        return 'default', 0.0

def main():
    """Main training function"""
    print("=" * 60)
    print("ğŸ¤– Training Intent Classification Model for AI Assistant")
    print("=" * 60)
    
    # Load training data
    print("\n1ï¸âƒ£ Loading training data...")
    texts, labels = load_training_data()
    
    if texts is None or len(texts) == 0:
        print("âš ï¸ No data from knowledge base, using synthetic data...")
        texts, labels = create_synthetic_training_data()
    
    # Add synthetic data for better coverage
    synthetic_texts, synthetic_labels = create_synthetic_training_data()
    texts.extend(synthetic_texts)
    labels.extend(synthetic_labels)
    
    print(f"âœ… Loaded {len(texts)} training examples")
    
    # Train model
    print("\n2ï¸âƒ£ Training model...")
    model, vectorizer = train_model(texts, labels, model_type='naive_bayes')
    
    if model is None:
        print("âŒ Training failed")
        return
    
    # Save model
    print("\n3ï¸âƒ£ Saving model...")
    save_model(model, vectorizer)
    
    # Test predictions
    print("\n4ï¸âƒ£ Testing predictions...")
    test_cases = [
        "how to identify butterflies?",
        "when is the best time to observe birds?",
        "how to take good photos?",
        "what species can you identify?",
        "hello",
        "ä½ å¥½",
        "å¦‚ä½•è­˜åˆ¥è´è¶ï¼Ÿ"
    ]
    
    for test_text in test_cases:
        intent, confidence = predict_intent(model, vectorizer, test_text)
        print(f"  '{test_text}' â†’ {intent} ({confidence:.2%})")
    
    print("\n" + "=" * 60)
    print("âœ… Training completed successfully!")
    print("=" * 60)

if __name__ == '__main__':
    main()

